{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm2BgSwrb61O"
      },
      "source": [
        "# Tugas Besar Pemroseran Parallel Kelas B\n",
        "## Analisis Data Games menggunakan MPI (reduce, filter), pyspark context (reduce, filter), dan sparksession (query) \n",
        "\n",
        "1. Rakasya Yoga Surya Prastama (F1D02310022)\n",
        "2. Rakasya Yoga Surya Prastama (F1D02310022)\n",
        "3. Rakasya Yoga Surya Prastama (F1D02310022)\n",
        "4. Rakasya Yoga Surya Prastama (F1D02310022)\n",
        "5. Rakasya Yoga Surya Prastama (F1D02310022)\n",
        "\n",
        "Link Kaggle untuk dataset: https://www.kaggle.com/datasets/jummyegg/rawg-game-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdnGA0QqYQar",
        "outputId": "684200c8-3197-4e71-f70a-5654e821bcf5"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark\n",
        "%pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baris kode ini Menginstall 2 library yang akan digunakan, yaitu 'pyspark' dan juga 'mpi4py' dengan perintah '%pip install. 'pyspark' digunakan untuk menjalankan Spark di lingkungan Python, yang memungkinkan pemrosesan data besar secara paralel dan terdistribusi. Sedangkan 'mpi4py' adalah antarmuka Python untuk MPI (Message Passing Interface), yang digunakan untuk membagi dan mengelola tugas antar proses dalam pemrograman paralel manual. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csNk3HyffLnd",
        "outputId": "0cff6aa7-cdd7-440d-db33-08517a1cc568"
      },
      "outputs": [],
      "source": [
        "from mpi4py import MPI\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baris kode ini memuat semua library yang akan digunakan untuk projek ini. 'mpi4py.MPI' digunakan untuk komunikasi antar proses saat menggunakan 'MPI', sedangkan 'SparkSession' dari 'pyspark.sql' digunakan untuk membaca dan memproses data tabular seperti database. Library tambahan seperti 'pandas', 'numpy', dan 'csv' juga diimpor, kemungkinan untuk manipulasi data dan pembacaan file CSV di luar konteks Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li4C8f_4tS4S"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "mulai_ukur = time.time()\n",
        "akhir_ukur = time.time()\n",
        "\n",
        "waktu_pengerjaan = akhir_ukur - mulai_ukur\n",
        "\n",
        "print(waktu_pengerjaan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baris kode ini bertujuan mengukur durasi proses yang akan dijalankan. Namun, karena waktu mulai yaitu 'mulai_ukur' dan akhir yaitu 'akhir_ukur' dicatat tanpa proses di antaranya, nilai 'waktu_pengerjaan' akan mendekati nol. Jika nilai ditempatkan dengan benar di sekitar blok pemrosesan utama, pengukuran ini nantinya akan berguna untuk membandingkan kinerja MPI dan PySpark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4hRJCx2fPph",
        "outputId": "031b9bf7-3b62-4745-de48-e3cf55acdbd3"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Game Sales Analysis\").getOrCreate()\n",
        "data = spark.read.csv(\"game_info.csv\", header=True, inferSchema=True)\n",
        "data.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada baris kode ini 'SparkSession' dibuat dengan nama aplikasi \"Game Sales Analysis\", lalu digunakan untuk membaca file CSV 'game_info.csv'. File dibaca sebagai DataFrame dengan parameter 'header=True' agar baris pertama digunakan sebagai nama kolom dan 'inferSchema=True' agar tipe data dikenali otomatis. Kemudian, sepuluh baris pertama dari data ditampilkan untuk keperluan verifikasi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mpi = MPI.COMM_WORLD\n",
        "size = mpi.Get_size()\n",
        "\n",
        "df = pd.read_csv(\"game_info.csv\")\n",
        "chunks = [df.iloc[i::size].reset_index(drop=True) for i in range(size)]\n",
        "\n",
        "# Broadcast potongan data\n",
        "scatter = mpi.scatter(chunks, root=0)\n",
        "\n",
        "# FILTER: metacritic > 80\n",
        "Filter = scatter[scatter[\"metacritic\"] > 80]\n",
        "\n",
        "# REDUCE: jumlah ratings_count\n",
        "Reduce = Filter[\"ratings_count\"].sum()\n",
        "total_Reduce = mpi.reduce(Reduce, op=MPI.SUM, root=0)\n",
        "\n",
        "print(\"Total ratings_count untuk metacritic > 80:\", total_Reduce)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada baris Kode ini dengan Menggunakan mpi4py, data dibaca melalui 'pandas' dan dibagi menjadi beberapa potongan berdasarkan jumlah proses (size). Setiap proses mendapatkan bagian data untuk dianalisis secara paralel menggunakan scatter. Filter dilakukan untuk memilih data dengan nilai metacritic > 80, kemudian dilakukan reduce untuk menghitung total ratings_count, dan hasil akhirnya dikumpulkan di root process menggunakan mpi.reduce."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buat SparkSession dan SparkContext\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Baca file CSV sebagai RDD\n",
        "rdd = sc.textFile(\"game_info.csv\")\n",
        "\n",
        "# Ambil header\n",
        "header = rdd.first()\n",
        "\n",
        "# Hilangkan header dan parsing CSV\n",
        "data = rdd.filter(lambda x: x != header).map(lambda row: next(csv.reader([row])))\n",
        "\n",
        "# FILTER: metacritic > 80 (kolom ke-4, index 3)\n",
        "filtered = data.filter(lambda x: len(x) > 3 and x[3].isdigit() and int(x[3]) > 80)\n",
        "\n",
        "# REDUCE: total ratings_count (kolom ke-13, index 12 â€” sesuaikan jika struktur berubah)\n",
        "ratings_count_index = 12\n",
        "ratings = filtered.map(lambda x: int(x[ratings_count_index]) if len(x) > ratings_count_index and x[ratings_count_index].isdigit() else 0)\n",
        "\n",
        "# Totalkan dengan reduce\n",
        "total = ratings.reduce(lambda a, b: a + b)\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(\"Total ratings_count (PySpark RDD):\", total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pada baris kode ini Data CSV dibaca sebagai RDD menggunakan 'SparkContext', dan 'header' dihapus dari 'data'. Setiap baris diubah menjadi list menggunakan parser CSV, kemudian difilter agar hanya menyisakan baris dengan metacritic > 80. Nilai 'ratings_count' dikonversi menjadi integer dan dijumlahkan menggunakan fungsi reduce. Hasil akhirnya adalah total 'ratings_count' dari data yang telah difilter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = spark.read.csv(\"game_info.csv\", header=True, inferSchema=True)\n",
        "data.createOrReplaceTempView(\"games\")\n",
        "\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT name, rating, metacritic, platforms, released, genres, publishers \n",
        "    FROM games\n",
        "    WHERE \n",
        "        rating IS NOT NULL AND \n",
        "        metacritic IS NOT NULL AND\n",
        "        TRY_CAST(rating AS FLOAT) IS NOT NULL AND \n",
        "        metacritic IS NOT NULL\n",
        "    ORDER BY rating  DESC\n",
        "    LIMIT 20\n",
        "\"\"\")\n",
        "\n",
        "print(\"Hasil Query:\")\n",
        "result.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Baris kode ini menggunakan fitur SQL dari Spark untuk menjalankan kueri terhadap DataFrame yang telah dibuat sebelumnya. Data difilter agar hanya menampilkan game yang memiliki nilai rating dan metacritic yang valid, lalu diurutkan berdasarkan rating secara menurun. Hasilnya dibatasi hanya 20 game teratas dan ditampilkan sebagai DataFrame 'pandas' agar lebih mudah dibaca."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
